<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png"><link rel="icon" href="/img/fluid.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="VestJin---靳马珏"><meta name="keywords" content="记录 博客 日志"><meta name="description" content="本地知识库聊天助手项目文档     项目概述 本项目旨在开发一个本地部署的智能知识库聊天助手，用户可以通过上传企业文档（如 PDF、Word 等），并通过自然语言提问来查询文档中的信息。系统使用本地部署的大语言模型 Ollama 提供语义理解，结合文档的向量化检索技术（使用 FAISS）来提高问答的准确性和效率。整个项目的架构充分考虑了数据隐私与安全，支持离线操作。  项目功能  文档上传与处"><meta property="og:type" content="article"><meta property="og:title" content="本地知识库聊天助手（基于DeepSeek模型）"><meta property="og:url" content="http://blog.jinmajue.site/posts/3fd23ce3/index.html"><meta property="og:site_name" content="靳马珏日志"><meta property="og:description" content="本地知识库聊天助手项目文档     项目概述 本项目旨在开发一个本地部署的智能知识库聊天助手，用户可以通过上传企业文档（如 PDF、Word 等），并通过自然语言提问来查询文档中的信息。系统使用本地部署的大语言模型 Ollama 提供语义理解，结合文档的向量化检索技术（使用 FAISS）来提高问答的准确性和效率。整个项目的架构充分考虑了数据隐私与安全，支持离线操作。  项目功能  文档上传与处"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2025-01-14T10:10:09.000Z"><meta property="article:modified_time" content="2025-04-14T15:42:15.265Z"><meta property="article:author" content="VestJin---靳马珏"><meta property="article:tag" content="记录 博客 日志"><meta name="twitter:card" content="summary_large_image"><meta name="referrer" content="no-referrer-when-downgrade"><title>本地知识库聊天助手（基于DeepSeek模型） - 靳马珏日志</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"blog.jinmajue.site",root:"/",version:"1.9.8",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:"§"},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:{measurement_id:null},tencent:{sid:null,cid:null},leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1},umami:{src:null,website_id:null,domains:null,start_time:"2023-08-30T00:00:00.000Z",token:null,api_server:null}},search_path:"/local-search.xml",include_content_in_search:!0};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="靳马珏日志" type="application/atom+xml">

<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>靳马珏的网络日志</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/" target="_self"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/" target="_self"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/default.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="本地知识库聊天助手（基于DeepSeek模型）"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2025-01-14 18:10" pubdate>2025年1月14日 晚上</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 3.8k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 32 分钟 </span><span id="busuanzi_container_page_pv" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="busuanzi_value_page_pv"></span> 次</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">本地知识库聊天助手（基于DeepSeek模型）</h1><div class="markdown-body"><hr><h2 id="本地知识库聊天助手项目文档"><a class="markdownIt-Anchor" href="#本地知识库聊天助手项目文档"></a> <strong>本地知识库聊天助手项目文档</strong></h2><hr><h1><a class="markdownIt-Anchor" href="#"></a></h1><h2 id="项目概述"><a class="markdownIt-Anchor" href="#项目概述"></a> 项目概述</h2><p>本项目旨在开发一个本地部署的智能知识库聊天助手，用户可以通过上传企业文档（如 PDF、Word 等），并通过自然语言提问来查询文档中的信息。系统使用本地部署的大语言模型 <strong>Ollama</strong> 提供语义理解，结合文档的向量化检索技术（使用 <strong>FAISS</strong>）来提高问答的准确性和效率。整个项目的架构充分考虑了数据隐私与安全，支持离线操作。</p><h2 id="项目功能"><a class="markdownIt-Anchor" href="#项目功能"></a> 项目功能</h2><ul><li><strong>文档上传与处理</strong>：用户可以上传 PDF、Word 等格式的文档，系统会自动解析并提取文本内容。</li><li><strong>本地语义理解</strong>：通过 <strong>Ollama</strong> 本地化部署的语言模型，理解用户提问并生成合理的回答。</li><li><strong>向量化检索</strong>：文档内容经过<strong>Sentence-BERT</strong>向量化处理后，存储在 <strong>FAISS</strong> 索引中，通过向量相似度检索最相关的答案。</li><li><strong>用户交互</strong>：提供一个简单的前端界面，用户可以方便地上传文档、输入问题并查看系统的回答。</li></ul><h2 id="技术选型与详细介绍"><a class="markdownIt-Anchor" href="#技术选型与详细介绍"></a> 技术选型与详细介绍</h2><h3 id="1-后端框架python-flask"><a class="markdownIt-Anchor" href="#1-后端框架python-flask"></a> 1. <strong>后端框架：Python + Flask</strong></h3><h4 id="flask-框架概述"><a class="markdownIt-Anchor" href="#flask-框架概述"></a> <strong>Flask 框架概述：</strong></h4><ul><li><strong>Flask</strong> 是一个用 Python 编写的轻量级 Web 框架。它提供了灵活的路由和丰富的扩展，可以快速构建 RESTful API 服务。</li><li>Flask 不强制使用特定的数据库或前端框架，能够根据项目需求灵活调整。</li></ul><h4 id="技术选择原因"><a class="markdownIt-Anchor" href="#技术选择原因"></a> <strong>技术选择原因：</strong></h4><ul><li><strong>易用性</strong>：Flask 的学习曲线较低，适合小型和中型应用的快速开发。</li><li><strong>扩展性</strong>：Flask 可以通过添加第三方库来扩展功能，比如数据库支持、用户认证、CORS 支持等，非常适合我们需要构建的简单且高效的 API 服务。</li><li><strong>与 Python 的兼容性</strong>：Flask 与 Python 科学计算库（如 Pandas、NumPy）兼容性好，便于与机器学习模型结合使用。</li></ul><h4 id="功能应用"><a class="markdownIt-Anchor" href="#功能应用"></a> <strong>功能应用：</strong></h4><ul><li>提供一个 API 接口，用于接收用户上传的文档文件。</li><li>提供 API 接口，接收用户提问并返回通过 <strong>Ollama</strong> 生成的答案。</li><li>提供前端页面支持的接口。</li></ul><h4 id="示例代码"><a class="markdownIt-Anchor" href="#示例代码"></a> <strong>示例代码：</strong></h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> flask <span class="hljs-keyword">import</span> Flask, request, jsonify<br><span class="hljs-keyword">import</span> ollama<br><span class="hljs-keyword">import</span> faiss<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>app = Flask(__name__)<br><br><span class="hljs-comment"># 假设已有一个 FAISS 向量索引</span><br>index = faiss.IndexFlatL2(<span class="hljs-number">512</span>)  <span class="hljs-comment"># 512 维的向量索引</span><br><br><span class="hljs-meta">@app.route(<span class="hljs-params"><span class="hljs-string">'/upload'</span>, methods=[<span class="hljs-string">'POST'</span>]</span>)</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">upload_document</span>():<br>    file = request.files[<span class="hljs-string">'document'</span>]<br>    <span class="hljs-comment"># 处理文件并提取文本（PDF、Word 等）</span><br>    text = process_document(file)<br>    <span class="hljs-comment"># 将文本向量化并存储到 FAISS 索引中</span><br>    vectors = vectorize_text(text)<br>    index.add(vectors)<br>    <span class="hljs-keyword">return</span> jsonify({<span class="hljs-string">"message"</span>: <span class="hljs-string">"Document uploaded successfully!"</span>})<br><br><span class="hljs-meta">@app.route(<span class="hljs-params"><span class="hljs-string">'/ask'</span>, methods=[<span class="hljs-string">'POST'</span>]</span>)</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">ask_question</span>():<br>    user_question = request.json.get(<span class="hljs-string">'question'</span>)<br>    question_vector = vectorize_text(user_question)<br>    <span class="hljs-comment"># 在 FAISS 索引中搜索最相似的文档片段</span><br>    _, indices = index.search(np.array([question_vector]), <span class="hljs-number">1</span>)<br>    <span class="hljs-comment"># 根据索引返回相关文档</span><br>    answer = retrieve_answer_from_index(indices)<br>    <span class="hljs-keyword">return</span> jsonify({<span class="hljs-string">'answer'</span>: answer})<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:<br>    app.run(debug=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></tbody></table></figure><hr><h3 id="2-本地语言模型ollama"><a class="markdownIt-Anchor" href="#2-本地语言模型ollama"></a> 2. <strong>本地语言模型：Ollama</strong></h3><h4 id="ollama-概述"><a class="markdownIt-Anchor" href="#ollama-概述"></a> <strong>Ollama 概述：</strong></h4><ul><li><strong>Ollama</strong> 是一个支持多种大型语言模型的本地推理工具，提供了简易的接口来调用预训练的语言模型。</li><li>使用 Ollama 可以方便地在本地机器上部署和运行如 LLaMA、Mistral 等大语言模型，避免了使用云服务的隐私问题。</li></ul><p>DeepSeek 是由国内团队开源的一系列大语言模型，分为几个方向：</p><ul><li><strong>DeepSeek-LLM</strong>：通用对话、知识问答</li><li><strong>DeepSeek-Coder</strong>：代码理解与生成</li><li><strong>DeepSeek-VL</strong>：多模态，支持图文理解</li></ul><p>具有较强的中文理解能力；本地部署无版权问题；与 ChatGPT 效果相近，但更适合中文和私有化部署</p><h4 id="技术选择原因-2"><a class="markdownIt-Anchor" href="#技术选择原因-2"></a> <strong>技术选择原因：</strong></h4><ul><li><strong>隐私保护</strong>：所有数据和模型都部署在本地，确保用户的文档和问题不会被上传至云端，提升数据隐私性。</li><li><strong>高效性</strong>：Ollama 提供简洁的 API，能够高效地调用语言模型进行推理，响应速度快，适合构建问答系统。</li><li><strong>支持多种模型</strong>：Ollama 支持多种开源预训练模型，如 LLaMA、Mistral 等，能够根据项目需求选择最合适的模型。</li></ul><h4 id="功能应用-2"><a class="markdownIt-Anchor" href="#功能应用-2"></a> <strong>功能应用：</strong></h4><ul><li>使用 Ollama 对用户的提问进行推理，生成合理的回答。</li><li>集成 Ollama 模型到 Flask 后端，处理提问并与向量检索结果结合提供精确答案。</li></ul><h4 id="示例代码-2"><a class="markdownIt-Anchor" href="#示例代码-2"></a> <strong>示例代码：</strong></h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> ollama<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_answer_from_ollama</span>(<span class="hljs-params">question</span>):<br>    <span class="hljs-comment"># 调用 Ollama 模型生成回答</span><br>    response = ollama.chat(prompt=question)<br>    <span class="hljs-keyword">return</span> response<br></code></pre></td></tr></tbody></table></figure><hr><h3 id="3-文档处理模块pdf-word-等格式的处理"><a class="markdownIt-Anchor" href="#3-文档处理模块pdf-word-等格式的处理"></a> 3. <strong>文档处理模块：PDF、Word 等格式的处理</strong></h3><h4 id="文档处理概述"><a class="markdownIt-Anchor" href="#文档处理概述"></a> <strong>文档处理概述：</strong></h4><ul><li><strong>文档处理模块</strong>负责从上传的文件（如 PDF、Word）中提取文本，并进行清理和预处理，保证提取的文本可以用于后续的向量化和检索操作。</li><li>这一步非常重要，因为清洗后的文本会直接影响后续语义检索的质量。</li></ul><h4 id="技术选型"><a class="markdownIt-Anchor" href="#技术选型"></a> <strong>技术选型：</strong></h4><ul><li><strong>pdfminer.six</strong>：用于从 PDF 文件中提取文本。</li><li><strong>python-docx</strong>：用于从 Word 文件中提取内容。</li><li><strong>pypandoc</strong>：用于转换其他格式文件（如 Markdown、HTML）为纯文本格式。</li><li><strong>正则表达式和分段技术</strong>：处理多余的内容（如页码、标题、页眉等），分段或按一定长度切分文档。</li></ul><h4 id="功能应用-3"><a class="markdownIt-Anchor" href="#功能应用-3"></a> <strong>功能应用：</strong></h4><ul><li>提取 PDF、Word 等格式的文档内容，清理无关信息，如页码、标题等。</li><li>对文档中的内容进行结构化处理，确保文本内容准确，适合向量化处理。</li></ul><h4 id="示例代码-3"><a class="markdownIt-Anchor" href="#示例代码-3"></a> <strong>示例代码：</strong></h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> pdfminer.high_level <span class="hljs-keyword">import</span> extract_text<br><span class="hljs-keyword">from</span> docx <span class="hljs-keyword">import</span> Document<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">extract_text_from_pdf</span>(<span class="hljs-params">file_path</span>):<br>    <span class="hljs-keyword">return</span> extract_text(file_path)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">extract_text_from_docx</span>(<span class="hljs-params">file_path</span>):<br>    doc = Document(file_path)<br>    text = <span class="hljs-string">'\n'</span>.join([para.text <span class="hljs-keyword">for</span> para <span class="hljs-keyword">in</span> doc.paragraphs])<br>    <span class="hljs-keyword">return</span> text<br></code></pre></td></tr></tbody></table></figure><hr><h3 id="4-向量化检索faiss"><a class="markdownIt-Anchor" href="#4-向量化检索faiss"></a> 4. <strong>向量化检索：FAISS</strong></h3><h4 id="faiss-概述"><a class="markdownIt-Anchor" href="#faiss-概述"></a> <strong>FAISS 概述：</strong></h4><ul><li><strong>FAISS (Facebook AI Similarity Search)</strong> 是一个由 Facebook AI 研究团队开发的高效相似度搜索库。它是一个用于高维向量数据检索的开源库，专门设计来进行大规模向量搜索任务，能够处理数百万甚至数十亿的向量数据。</li><li>FAISS 通过高效的近似最近邻算法（ANN），在大量向量数据中快速找到与查询向量最相似的向量，广泛应用于图像检索、语音识别、文本检索等任务。适用于本项目中的文档检索。</li></ul><h4 id="技术选择原因-3"><a class="markdownIt-Anchor" href="#技术选择原因-3"></a> <strong>技术选择原因：</strong></h4><ul><li><strong>高效性</strong>：FAISS 提供高效的相似度搜索算法，能够处理数百万级别的文档向量，支持多维向量索引。</li><li><strong>灵活性</strong>：FAISS 支持不同类型的索引结构，如平面索引、倒排索引等，可以根据需求选择最合适的索引方式。</li></ul><h4 id="功能应用-4"><a class="markdownIt-Anchor" href="#功能应用-4"></a> <strong>功能应用：</strong></h4><ul><li>对上传的文档内容进行向量化处理，将这些向量添加到 FAISS 索引中，使用 FAISS 建立向量数据库。</li><li>当用户提问时，系统会将问题向量化，并在 FAISS 索引中查找最相似的文档内容，提升检索精度。</li></ul><h4 id="示例代码-4"><a class="markdownIt-Anchor" href="#示例代码-4"></a> <strong>示例代码：</strong></h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> faiss<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sentence_transformers <span class="hljs-keyword">import</span> SentenceTransformer<br><br><span class="hljs-comment"># 使用预训练模型进行文本向量化</span><br>model = SentenceTransformer(<span class="hljs-string">'paraphrase-MiniLM-L6-v2'</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode_texts</span>(<span class="hljs-params">texts</span>):<br>    <span class="hljs-keyword">return</span> model.encode(texts)<br><br><span class="hljs-comment"># 构建 FAISS 索引</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_faiss_index</span>(<span class="hljs-params">vectors</span>):<br>    index = faiss.IndexFlatL2(vectors.shape[<span class="hljs-number">1</span>])  <span class="hljs-comment"># 创建一个 L2 距离的索引</span><br>    index.add(vectors)  <span class="hljs-comment"># 向索引添加向量</span><br>    <span class="hljs-keyword">return</span> index<br><br><span class="hljs-comment"># 搜索相似文本</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">search_similar</span>(<span class="hljs-params">query_vector, index, k=<span class="hljs-number">5</span></span>):<br>    _, indices = index.search(np.array([query_vector]), k)<br>    <span class="hljs-keyword">return</span> indices<br></code></pre></td></tr></tbody></table></figure><hr><h3 id="5-前端vue3-element-plus"><a class="markdownIt-Anchor" href="#5-前端vue3-element-plus"></a> 5. <strong>前端：Vue3 + Element Plus</strong></h3><h4 id="vue3-和-element-plus-概述"><a class="markdownIt-Anchor" href="#vue3-和-element-plus-概述"></a> <strong>Vue3 和 Element Plus 概述：</strong></h4><ul><li><strong>Vue3</strong> 是一种现代的 JavaScript 框架，支持组件化开发、响应式数据绑定等特性，适用于构建交互性强的 Web 应用。</li><li><strong>Element Plus</strong> 是基于 Vue3 的 UI 组件库，提供了丰富的 UI 元素，如按钮、输入框、表格等，能够帮助快速构建高质量的用户界面。</li></ul><h4 id="功能应用-5"><a class="markdownIt-Anchor" href="#功能应用-5"></a> <strong>功能应用：</strong></h4><ul><li>提供用户友好的前端界面，用户可以方便地上传文档、提问问题并查看回答。</li><li>支持用户输入问题，后台通过 Flask 调用 Ollama 模型和向量检索返回答案。</li></ul><h4 id="示例代码-5"><a class="markdownIt-Anchor" href="#示例代码-5"></a> <strong>示例代码：</strong></h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs vue">&lt;template&gt;<br>  &lt;div&gt;<br>    &lt;el-upload<br>      action="/upload"<br>      :on-success="handleUploadSuccess"<br>      :before-upload="beforeUpload"<br>    &gt;<br>      &lt;el-button&gt;上传文档&lt;/el-button&gt;<br>    &lt;/el-upload&gt;<br>    &lt;el-input v-model="question" placeholder="请输入问题"&gt;&lt;/el-input&gt;<br>    &lt;el-button @click="askQuestion"&gt;提问&lt;/el-button&gt;<br>    &lt;el-card v-if="answer"&gt;<br>      &lt;p&gt;{{ answer }}&lt;/p&gt;<br>    &lt;/el-card&gt;<br>  &lt;/div&gt;<br>&lt;/template&gt;<br><br>&lt;script&gt;<br>export default {<br>  data() {<br>    return {<br>      question: '',<br>      answer: '',<br>    };<br>  },<br>  methods: {<br>    async askQuestion() {<br>      const response = await this.$http.post('/ask', { question: this.question });<br>      this.answer = response.data.answer;<br>    },<br>    handleUploadSuccess() {<br>      this.$message.success('文件上传成功');<br>    },<br>    beforeUpload(file) {<br>      // 检查文件类型<br>      return file.type === 'application/pdf';<br>    }<br>  }<br>};<br>&lt;/script&gt;<br></code></pre></td></tr></tbody></table></figure><hr><h3 id="五-应用场景"><a class="markdownIt-Anchor" href="#五-应用场景"></a> 五、应用场景</h3><p>为企业内部人员提供了便捷的知识库查询工具，大幅提高了工作效率，减少了人工查询的时间成本。</p><p>本地部署方案确保了数据的安全性和隐私性，满足了严格的企业级需求。</p><p>提供了灵活的扩展性，系统可根据不同需求进行二次开发和集成，适用于各类行业场景（如企业管理、法律咨询、教育等）。</p><table><thead><tr><th>场景</th><th>描述</th></tr></thead><tbody><tr><td>💼 企业内部文档助手</td><td>向系统提问公司规章、流程说明、产品文档</td></tr><tr><td>🧑‍🎓 教育知识库助教</td><td>学生上传课程资料，快速定位重点、提问复习</td></tr><tr><td>🏛️ 政策法规问答平台</td><td>政府机关内部文件语义搜索与问答</td></tr><tr><td>📑 法律/合同文档助手</td><td>快速理解长篇合同、政策材料内容含义</td></tr></tbody></table><h3 id="六-扩展功能以后"><a class="markdownIt-Anchor" href="#六-扩展功能以后"></a> 六、扩展功能（以后）</h3><ul><li>✅ 多用户登录系统，支持知识权限管理</li><li>✅ 增加 Web 界面（Flask + Vue）支持上传/提问/反馈</li><li>✅ 用户提问日志记录与反馈优化模型</li><li>✅ 与企业 OA 系统或微信群机器人打通，实现消息集成</li></ul><h3 id="七-逻辑框架"><a class="markdownIt-Anchor" href="#七-逻辑框架"></a> 七、逻辑框架</h3><ol><li><strong>项目目标：</strong> 离线构建智能知识问答助手，服务特定用户/企业场景</li><li><strong>为何选用本地模型：</strong> 数据私密、可控稳定、无需联网</li><li><strong>技术结构：</strong> 文档预处理 ➜ 向量化 ➜ DeepSeek 回答</li><li><strong>关键亮点：</strong> 支持多格式文档、中文向量搜索、部署灵活</li><li><strong>扩展方向：</strong> 聊天增强、问答评分、语音问答、多用户管理</li></ol><p>✅ 为什么我们要将文档转成向量？</p><p>✅ 本地模型和云端模型相比有什么优势？</p><p>✅ RAG 是什么，它比直接问 ChatGPT 好在哪？</p><p>✅ FAISS + Embedding 是解决搜索问题的“脑子”</p><p>✅ DeepSeek 是我们离线“嘴巴”——懂中文、能回答</p><hr><h2 id="sentence-bert-详细介绍"><a class="markdownIt-Anchor" href="#sentence-bert-详细介绍"></a> <strong>Sentence-BERT 详细介绍</strong></h2><p><strong>Sentence-BERT</strong>（简称 <strong>SBERT</strong>）是一个基于 BERT（Bidirectional Encoder Representations from Transformers）的改进版本，专门用于生成句子的向量表示（即嵌入）。BERT 本身是一个强大的预训练语言模型，可以生成单词的向量表示，但它的设计是为了处理单个单词的上下文，而不是生成句子的整体语义向量。为了有效地对句子进行表示，<strong>Sentence-BERT</strong> 提出了一个改进的方法，通过对 BERT 进行微调，使其可以更好地生成句子的嵌入。</p><h4 id="1-背景"><a class="markdownIt-Anchor" href="#1-背景"></a> <strong>1. 背景</strong></h4><p>BERT 是通过上下文学习单词的表示，主要关注的是 <strong>Masked Language Modeling (MLM)</strong> 和 <strong>Next Sentence Prediction (NSP)</strong>，这使得 BERT 非常适合处理单词和短文本的上下文。但在实际应用中，我们常常需要对整个句子、段落或文档进行表示。BERT 的默认输出是 <strong>每个 Token（单词或子词）的向量</strong>，而不是对整个句子的语义进行编码。</p><p>为了解决这个问题，<strong>Sentence-BERT</strong> 通过以下方式对 BERT 进行优化：</p><ol><li><strong>句子对微调（Siamese Network 或 Triplet Network）：</strong><ul><li><strong>Siamese Network</strong> 采用双塔结构，将同一文本的两个版本（如原始句子和修改过的句子）输入到 BERT 中，通过相似度来训练模型，使得模型学习到能将语义相似的句子映射到相近的向量空间。</li><li><strong>Triplet Network</strong> 则使用三元组训练，增加了一个负样本，进一步提升句子表示的区分度。</li></ul></li><li><strong>通过特定任务进行微调：</strong><ul><li>例如，Sentence-BERT 通常通过 <strong>自然语言推理（NLI）</strong> 和 <strong>对比学习任务</strong>（比如通过训练相似句子的对比）进行优化，从而提高生成句子向量时的准确性。</li></ul></li><li><strong>输出固定维度的句子向量：</strong><ul><li>在传统的 BERT 中，输入的是词或子词的向量，输出的是 <strong>Token Embeddings</strong>。而 Sentence-BERT 将句子转化为一个固定维度的向量（通常是 768 或 512 维），便于后续的相似度计算、检索等任务。</li></ul></li></ol><h4 id="2-sentence-bert-的优势"><a class="markdownIt-Anchor" href="#2-sentence-bert-的优势"></a> <strong>2. Sentence-BERT 的优势</strong></h4><ul><li><strong>高效的句子表示：</strong> 在原始 BERT 中，为了获得句子嵌入，通常需要对整个句子进行 [CLS] token 以及其他 tokens 的向量处理，而 <strong>Sentence-BERT</strong> 通过微调提供了更简洁高效的生成句子向量的方法。</li><li><strong>适合文本匹配：</strong> <strong>Sentence-BERT</strong> 特别适用于句子对任务（如文本匹配、语义相似度计算），能够高效地对句子进行编码并进行对比。</li><li><strong>高效的相似度检索：</strong> 由于 Sentence-BERT 输出的是固定维度的向量，因此可以用来构建向量检索系统，利用如 FAISS 等工具快速进行检索。</li></ul><h4 id="3-sentence-bert-的应用场景"><a class="markdownIt-Anchor" href="#3-sentence-bert-的应用场景"></a> <strong>3. Sentence-BERT 的应用场景</strong></h4><ul><li><strong>语义文本相似度计算：</strong> 判断两段文本之间的语义相似度，广泛应用于搜索引擎、问答系统、推荐系统等领域。</li><li><strong>文本匹配任务：</strong> 比如句子对分类任务、语义相似度评分、翻译质量评估等。</li><li><strong>信息检索：</strong> 用于基于文本向量进行文档检索，如根据问题查询最相关的文档或答案。</li><li><strong>聚类分析：</strong> 将文本转换为向量后，可以利用聚类算法对相似的文本进行分组。</li></ul><hr><h3 id="4-sentence-bert-使用"><a class="markdownIt-Anchor" href="#4-sentence-bert-使用"></a> <strong>4. Sentence-BERT 使用</strong></h3><h4 id="安装-sentence-bert"><a class="markdownIt-Anchor" href="#安装-sentence-bert"></a> <strong>安装 Sentence-BERT</strong></h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install sentence-transformers<br></code></pre></td></tr></tbody></table></figure><h4 id="基本用法"><a class="markdownIt-Anchor" href="#基本用法"></a> <strong>基本用法：</strong></h4><ol><li><strong>加载模型：</strong> 可以加载 <code>Sentence-BERT</code> 提供的多个预训练模型，也可以使用自己的数据进行微调。</li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sentence_transformers <span class="hljs-keyword">import</span> SentenceTransformer<br><br><span class="hljs-comment"># 加载预训练模型</span><br>model = SentenceTransformer(<span class="hljs-string">'paraphrase-MiniLM-L6-v2'</span>)<br></code></pre></td></tr></tbody></table></figure><ol><li><strong>文本向量化：</strong></li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 示例句子</span><br>sentences = [<span class="hljs-string">"我喜欢自然语言处理"</span>, <span class="hljs-string">"深度学习在AI领域应用广泛"</span>]<br><br><span class="hljs-comment"># 使用 Sentence-BERT 将文本转化为向量</span><br>sentence_embeddings = model.encode(sentences)<br><br><span class="hljs-comment"># 输出句子嵌入</span><br><span class="hljs-keyword">for</span> i, sentence <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(sentences):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Sentence: <span class="hljs-subst">{sentence}</span>"</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Embedding: <span class="hljs-subst">{sentence_embeddings[i]}</span>"</span>)<br></code></pre></td></tr></tbody></table></figure><ol><li><strong>文本相似度计算：</strong><ul><li>通过计算两个句子向量的 <strong>余弦相似度</strong> 来判断它们的相似度。</li></ul></li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.metrics.pairwise <span class="hljs-keyword">import</span> cosine_similarity<br><br><span class="hljs-comment"># 计算相似度</span><br>similarity = cosine_similarity([sentence_embeddings[<span class="hljs-number">0</span>]], [sentence_embeddings[<span class="hljs-number">1</span>]])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f"Cosine similarity: <span class="hljs-subst">{similarity[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]}</span>"</span>)<br></code></pre></td></tr></tbody></table></figure><ol><li><strong>保存与加载模型：</strong><ul><li><strong>保存模型：</strong></li></ul></li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.save(<span class="hljs-string">'my_model'</span>)<br></code></pre></td></tr></tbody></table></figure><ul><li><strong>加载已保存的模型：</strong></li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model = SentenceTransformer(<span class="hljs-string">'my_model'</span>)<br></code></pre></td></tr></tbody></table></figure><h4 id="5-微调-sentence-bert"><a class="markdownIt-Anchor" href="#5-微调-sentence-bert"></a> <strong>5. 微调 Sentence-BERT</strong></h4><p>Sentence-BERT 还可以通过特定的数据集进行微调。比如，您可以使用一个包含成对句子及其相似度标签的数据集来微调模型，使得模型生成的句子向量更加符合您的需求。</p><ol><li><strong>准备数据：</strong> 你需要准备一对句子和它们的相似度标签。<ul><li>数据格式如下：每对句子有一个标签，通常是一个 0 到 1 之间的值，表示这对句子的相似度。</li></ul></li><li><strong>使用 <code>SentenceTransformer</code> 进行微调：</strong></li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sentence_transformers <span class="hljs-keyword">import</span> SentenceTransformer, SentencesDataset, losses<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br><span class="hljs-comment"># 加载预训练模型</span><br>model = SentenceTransformer(<span class="hljs-string">'paraphrase-MiniLM-L6-v2'</span>)<br><br><span class="hljs-comment"># 准备微调数据</span><br>train_samples = [<br>    (<span class="hljs-string">'我喜欢自然语言处理'</span>, <span class="hljs-string">'自然语言处理非常有趣'</span>, <span class="hljs-number">0.9</span>),<br>    (<span class="hljs-string">'我喜欢看电影'</span>, <span class="hljs-string">'我喜欢阅读书籍'</span>, <span class="hljs-number">0.3</span>),<br>    <span class="hljs-comment"># 更多的句子对和相似度</span><br>]<br><br><span class="hljs-comment"># 创建数据集</span><br>train_data = SentencesDataset(train_samples, model)<br>train_dataloader = DataLoader(train_data, batch_size=<span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 使用对比损失进行训练</span><br>train_loss = losses.CosineSimilarityLoss(model)<br>model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=<span class="hljs-number">1</span>, warmup_steps=<span class="hljs-number">100</span>)<br></code></pre></td></tr></tbody></table></figure><ol><li><strong>评估与推理：</strong></li></ol><p>微调后的模型可以通过 <code>encode</code> 方法生成句子向量，适用于相似度计算、文本检索等任务。</p><hr></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/projects/" class="category-chain-item">projects</a></span></span></div></div><div class="license-box my-3"><div class="license-title"><div>本地知识库聊天助手（基于DeepSeek模型）</div><div>http://blog.jinmajue.site/posts/3fd23ce3/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>VestJin---靳马珏</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2025年1月14日</div></div><div class="license-meta-item"><div>许可协议</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-cc-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/posts/16c889de/" title="施氏食狮史"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">施氏食狮史</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/posts/b4f97437/" title="关于SQLite的各种趣事"><span class="hidden-mobile">关于SQLite的各种趣事</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><script type="text/javascript">Fluid.utils.loadComments("#comments",(function(){var t=document.documentElement.getAttribute("data-user-color-scheme");t="dark"===t?"github-dark":"boxy-light",window.UtterancesThemeLight="boxy-light",window.UtterancesThemeDark="github-dark";var e=document.createElement("script");e.setAttribute("src","https://utteranc.es/client.js"),e.setAttribute("repo","Code-Mist/fluid-comment"),e.setAttribute("issue-term","pathname"),e.setAttribute("theme",t),e.setAttribute("crossorigin","anonymous"),document.getElementById("comments").appendChild(e)}))</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><script>Fluid.utils.createScript("https://lib.baomitu.com/mermaid/8.14.0/mermaid.min.js",(function(){mermaid.initialize({theme:"default"}),Fluid.utils.listenDOMLoaded((function(){Fluid.events.registerRefreshCallback((function(){"mermaid"in window&&mermaid.init()}))}))}))</script><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div class="statistics"><span id="busuanzi_container_site_pv" style="display:none">总访问量 <span id="busuanzi_value_site_pv"></span> 次 </span><span id="busuanzi_container_site_uv" style="display:none">总访客数 <span id="busuanzi_value_site_uv"></span> 人</span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script>window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise()):window.MathJax={tex:{inlineMath:{"[+]":[["$","$"]]}},loader:{load:["ui/lazy"]},options:{renderActions:{insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(t=>{let e=t.parentNode;"li"===e.nodeName.toLowerCase()&&e.parentNode.classList.add("has-jax")})},"",!1]}}},Fluid.events.registerRefreshCallback((function(){"MathJax"in window&&MathJax.startup.document&&"function"==typeof MathJax.startup.document.state&&(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise())}))</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script src="/js/local-search.js"></script><script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>